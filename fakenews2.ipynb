{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshvardhan981/Fake_News_Detector/blob/main/fakenews2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b630fdc6-08b8-4510-8cf6-c08df5f490ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b630fdc6-08b8-4510-8cf6-c08df5f490ca",
        "outputId": "e5063982-99cf-4ff1-e7b7-15fa7ec91b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Collecting peft\n",
            "  Downloading peft-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.13.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading transformers-4.51.0-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.15.1-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, transformers, datasets, accelerate, peft\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install --upgrade transformers peft accelerate huggingface-hub datasets\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer,AutoModel,TrainingArguments\n",
        "from transformers import AutoModelForSequenceClassification,AutoConfig\n",
        "from transformers import Trainer,BertModel,BertForSequenceClassification\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "from warnings import simplefilter\n",
        "simplefilter(\"ignore\")"
      ]
    },
    {
      "source": [
        "!pip install --upgrade jax jaxlib"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "5O0EUCc6RWCT",
        "outputId": "d5522d25-96cf-42d4-910c-1c2d657af792"
      },
      "id": "5O0EUCc6RWCT",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Collecting jax\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax) (1.14.1)\n",
            "Downloading jax-0.5.3-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl (105.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.1/105.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "Successfully installed jax-0.5.3 jaxlib-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jaxlib"
                ]
              },
              "id": "9b6fc2acd8a14966abb0bca304514c3e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff7a21d0-900f-4d0d-a4f2-16868507f8a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff7a21d0-900f-4d0d-a4f2-16868507f8a6",
        "outputId": "9d2461c8-344b-4b14-bd48-1c708e331969"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7bd59ac2-2a63-490c-9b71-3b50d4e4b0a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bd59ac2-2a63-490c-9b71-3b50d4e4b0a1",
        "outputId": "1df2021f-60a1-4b6a-d9b6-67b653418935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m22051453\u001b[0m (\u001b[33maratrika123\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "# Set API key as an environment variable (Recommended)\n",
        "os.environ[\"WANDB_API_KEY\"] = \"6674505d166876897d9dd5266ea47f1380fec9da\"\n",
        "\n",
        "# Log in to Weights & Biases\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e0f42abb-9ceb-4c6f-a771-20e9326309b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0f42abb-9ceb-4c6f-a771-20e9326309b5",
        "outputId": "5094cf23-6dfb-414c-a4bc-14a6b7cc551e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "load_dotenv()  # Load environment variables from .env file\n",
        "\n",
        "wandb.login(key=os.getenv(\"6674505d166876897d9dd5266ea47f1380fec9da\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29946842-f3ac-4c82-88b0-a0c181713f55",
      "metadata": {
        "id": "29946842-f3ac-4c82-88b0-a0c181713f55"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n",
        "\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data =pd.read_csv('https://zenodo.org/record/4561253/files/WELFake_Dataset.csv?download=1')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "79a2Hy4uFl5l"
      },
      "id": "79a2Hy4uFl5l",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check if the column 'label' exists before renaming (it was renamed in a previous step)\n",
        "if 'label' in data.columns:\n",
        "    # If it does exist, rename it to 'labels'\n",
        "    data.rename(columns={'label': 'labels'}, inplace=True)\n",
        "else:\n",
        "    # Otherwise, do nothing\n",
        "    pass\n",
        "\n",
        "# Proceed with the rest of your code\n",
        "news_map = {1: 'real', 0: 'fake'}\n",
        "data['label_names'] = data['labels'].map(news_map)\n",
        "\n",
        "sns.histplot(data=data.label_names)\n",
        "plt.title('Distribution of the Target Classes', fontsize=25)\n",
        "plt.xlabel('target classes', fontsize=15)\n",
        "plt.ylabel('count', fontsize=15)\n",
        "plt.tight_layout()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nHmi0JVRFPft"
      },
      "id": "nHmi0JVRFPft",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Check if the column 'Unnamed: 0' exists before dropping\n",
        "if 'Unnamed: 0' in data.columns:\n",
        "    data.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "\n",
        "# Proceed with renaming the column\n",
        "data.rename(columns={'label': 'labels'}, inplace=True)\n",
        "\n",
        "data.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OLSsexD8D45H"
      },
      "id": "OLSsexD8D45H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94077456-1106-4204-b388-b01923bbde36",
      "metadata": {
        "id": "94077456-1106-4204-b388-b01923bbde36"
      },
      "outputs": [],
      "source": [
        "data.isna().sum()"
      ]
    },
    {
      "source": [
        "def plot_nulls(data,title,x_axis_label,y_axis_label):\n",
        "\n",
        "    # Number of nulls for each column\n",
        "    # Get counts of True (null) values for each column\n",
        "    null_counts = data.isnull().sum()\n",
        "    # Calculate percentage of null values\n",
        "    data_nulls = (null_counts / len(data) * 100).reset_index(name='count')\n",
        "\n",
        "    sns.barplot(data_nulls,x=\"index\",y=\"count\")\n",
        "    plt.title(title,fontsize=20)\n",
        "    plt.xlabel(x_axis_label,fontsize=13)\n",
        "    plt.ylabel(y_axis_label,fontsize=13)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_nulls(data,\"Null Values in the Dataset\",'features','% of null values in the column')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JmEdQYxnELDE"
      },
      "id": "JmEdQYxnELDE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcf662c9-651c-4a3f-ba97-b63a6452e7d3",
      "metadata": {
        "id": "dcf662c9-651c-4a3f-ba97-b63a6452e7d3"
      },
      "outputs": [],
      "source": [
        "data.dropna(axis=0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "724fe7dc-5013-4f70-9ee1-7a39e6fb85c9",
      "metadata": {
        "id": "724fe7dc-5013-4f70-9ee1-7a39e6fb85c9"
      },
      "outputs": [],
      "source": [
        "data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8fc7d20-fb1b-42ff-a8db-94b462588bf7",
      "metadata": {
        "id": "e8fc7d20-fb1b-42ff-a8db-94b462588bf7"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ddbdc9-caef-48af-873b-cd18f133219e",
      "metadata": {
        "id": "93ddbdc9-caef-48af-873b-cd18f133219e"
      },
      "outputs": [],
      "source": [
        "data = data.sample(frac=1,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeed397e-0318-4e8b-99dd-2b5b9457329e",
      "metadata": {
        "id": "eeed397e-0318-4e8b-99dd-2b5b9457329e"
      },
      "outputs": [],
      "source": [
        "news_map = {1:'real',0:'fake'}\n",
        "data['label_names'] = data['labels'].map(news_map)\n",
        "\n",
        "sns.histplot(data=data.label_names)\n",
        "plt.title('Distribution of the Target Classes',fontsize=25)\n",
        "plt.xlabel('target classes',fontsize=15)\n",
        "plt.ylabel('count',fontsize=15)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "source": [
        "# Check if the column 'label' exists before renaming\n",
        "if 'label' in data.columns:\n",
        "    data.rename(columns={'label': 'labels'}, inplace=True)\n",
        "else:\n",
        "    print(\"Column 'label' not found in the DataFrame.\")\n",
        "\n",
        "# Proceed with the rest of your code\n",
        "news_map = {1:'real',0:'fake'}\n",
        "data['label_names'] = data['labels'].map(news_map)\n",
        "\n",
        "sns.histplot(data=data.label_names)\n",
        "plt.title('Distribution of the Target Classes',fontsize=25)\n",
        "plt.xlabel('target classes',fontsize=15)\n",
        "plt.ylabel('count',fontsize=15)\n",
        "plt.tight_layout()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Q8QwL4sYEdB2"
      },
      "id": "Q8QwL4sYEdB2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7bf41ef-02b7-4b1c-b9b2-4acee00af867",
      "metadata": {
        "id": "c7bf41ef-02b7-4b1c-b9b2-4acee00af867"
      },
      "outputs": [],
      "source": [
        "fake_title = data.loc[data.labels == 0]['title'].apply(lambda x: len(x.split()))\n",
        "real_title = data.loc[data.labels == 1]['title'].apply(lambda x: len(x.split()))\n",
        "\n",
        "data['title_length'] = data['title'].apply(lambda x: len(x.split()))\n",
        "\n",
        "avg_title = data.groupby('label_names')['title_length'].mean().reset_index(name='avg title length')\n",
        "##########################\n",
        "\n",
        "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(12,6))\n",
        "\n",
        "ax1 = sns.histplot(fake_title,ax=ax1,bins=6)\n",
        "ax1.set_xlim(0,24)\n",
        "ax1.set_xlabel('No. of words')\n",
        "ax1.set_title(\"No. of Words (Fake News Title)\",fontsize=14)\n",
        "\n",
        "ax2 = sns.histplot(real_title,ax=ax2,bins=15)\n",
        "ax2.set_xlim(0,30)\n",
        "ax2.set_xlabel('No. of words')\n",
        "ax2.set_title(\"No. of Words (Real News Title)\",fontsize=14)\n",
        "\n",
        "ax3 = sns.barplot(data=avg_title,x='label_names',y='avg title length')\n",
        "ax3.set_title(\"Avg No. of Words (Real vs Fake News Titles)\",fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "source": [
        "# Check if the column 'label' exists before renaming (it was renamed in a previous step)\n",
        "if 'label' in data.columns:\n",
        "    # If it does exist, rename it to 'labels'\n",
        "    data.rename(columns={'label': 'labels'}, inplace=True)\n",
        "else:\n",
        "    # Otherwise, do nothing\n",
        "    pass\n",
        "\n",
        "# Proceed with the rest of your code\n",
        "news_map = {1: 'real', 0: 'fake'}\n",
        "data['label_names'] = data['labels'].map(news_map)\n",
        "\n",
        "sns.histplot(data=data.label_names)\n",
        "plt.title('Distribution of the Target Classes', fontsize=25)\n",
        "plt.xlabel('target classes', fontsize=15)\n",
        "plt.ylabel('count', fontsize=15)\n",
        "plt.tight_layout()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BFoj7PsfEs1C"
      },
      "id": "BFoj7PsfEs1C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600c8d75-1b60-4658-a7fe-5f09b08830b3",
      "metadata": {
        "id": "600c8d75-1b60-4658-a7fe-5f09b08830b3"
      },
      "outputs": [],
      "source": [
        "text_fake = ' '.join(data.loc[data.labels == 0]['title'])\n",
        "text_real = ' '.join(data.loc[data.labels == 1]['title'])\n",
        "\n",
        "wordcloud_fake = WordCloud().generate(text_fake)\n",
        "wordcloud_real = WordCloud().generate(text_real)\n",
        "\n",
        "fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(12,6))\n",
        "\n",
        "ax1.imshow(wordcloud_fake)\n",
        "ax1.axis(\"off\")\n",
        "ax1.set_title(\"Wordcloud for the Fake News' Titles\",fontsize=20)\n",
        "\n",
        "ax2.imshow(wordcloud_real)\n",
        "ax2.axis(\"off\")\n",
        "ax2.set_title(\"Wordcloud for the 'Real' News' Titles\",fontsize=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c491165-d67c-4e29-a1ae-38c0a3b58842",
      "metadata": {
        "id": "6c491165-d67c-4e29-a1ae-38c0a3b58842"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Remove the corrupted model\n",
        "!python -m spacy validate\n",
        "!python -m spacy download en_core_web_sm --force\n",
        "\n",
        "# Try loading it again\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd54d27d-d346-46b5-9a0d-ec6cf8106a7f",
      "metadata": {
        "id": "cd54d27d-d346-46b5-9a0d-ec6cf8106a7f"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "data2 = data.copy()\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "data2['title'] = data2['title'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eee3fd3-aaa2-4aaf-a32d-41df0f974b2d",
      "metadata": {
        "id": "4eee3fd3-aaa2-4aaf-a32d-41df0f974b2d"
      },
      "outputs": [],
      "source": [
        "data2.drop(columns=['text','title_length'],axis=1,inplace=True)\n",
        "\n",
        "data2.rename(columns={'title':'text'},inplace=True)\n",
        "\n",
        "data2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56ce1219-992a-4fdb-b05c-8d2284363e04",
      "metadata": {
        "id": "56ce1219-992a-4fdb-b05c-8d2284363e04"
      },
      "outputs": [],
      "source": [
        "data3 = data2.copy()\n",
        "\n",
        "X = data3['text']\n",
        "y = data3['labels']\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "834bbe0a-e36c-406f-97b4-8b5136731556",
      "metadata": {
        "id": "834bbe0a-e36c-406f-97b4-8b5136731556"
      },
      "outputs": [],
      "source": [
        "# Create tokenization and modelling pipeline\n",
        "model_NB = Pipeline([\n",
        "                    (\"tfidf\",TfidfVectorizer()), # convert words to numbers using tfidf\n",
        "                    (\"clf\",MultinomialNB()) # model the text\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_NB.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44ae50df-1ce2-481a-ad94-3396f4cbbe86",
      "metadata": {
        "id": "44ae50df-1ce2-481a-ad94-3396f4cbbe86"
      },
      "outputs": [],
      "source": [
        "test_score = model_NB.score(X_test,y_test)\n",
        "\n",
        "test_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4acb35fc-1d80-40b4-b0e0-6c03454dfd24",
      "metadata": {
        "id": "4acb35fc-1d80-40b4-b0e0-6c03454dfd24"
      },
      "outputs": [],
      "source": [
        "y_pred = model_NB.predict(X_test)\n",
        "\n",
        "# Generate array of values for confusion matrix\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "ax = sns.heatmap(cm,annot=True)\n",
        "ax.xaxis.set_ticklabels(['fake news','real news'])\n",
        "ax.yaxis.set_ticklabels(['fake news','real news'])\n",
        "ax.set_title('Confusion Matrix - HF Transformers',fontsize=18)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e2a604-8756-4832-9b16-03cef1d46e96",
      "metadata": {
        "id": "e9e2a604-8756-4832-9b16-03cef1d46e96"
      },
      "outputs": [],
      "source": [
        "model_ckpt = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e90b898-1b8b-4322-be6f-466a15e2bd61",
      "metadata": {
        "id": "8e90b898-1b8b-4322-be6f-466a15e2bd61"
      },
      "outputs": [],
      "source": [
        "len(tokenizer.vocab), tokenizer.vocab_size, tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21e99157-6831-4038-bd29-3ce4467cf1c8",
      "metadata": {
        "id": "21e99157-6831-4038-bd29-3ce4467cf1c8"
      },
      "outputs": [],
      "source": [
        "train,test      = train_test_split(data2,test_size=0.3,stratify=data2['labels'])\n",
        "test,validation = train_test_split(test,test_size=1/3,stratify=test['labels'])\n",
        "\n",
        "train.shape, test.shape, validation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a7ff68a-3441-41d0-b24e-2ba59d8af1c9",
      "metadata": {
        "id": "7a7ff68a-3441-41d0-b24e-2ba59d8af1c9"
      },
      "outputs": [],
      "source": [
        "dataset = DatasetDict(\n",
        "    {'train':Dataset.from_pandas(train,preserve_index=False),\n",
        "     'test':Dataset.from_pandas(test,preserve_index=False),\n",
        "     'validation': Dataset.from_pandas(validation,preserve_index=False)\n",
        "     }\n",
        ")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ecdc94-92e5-4f0f-a894-1888856ec709",
      "metadata": {
        "id": "85ecdc94-92e5-4f0f-a894-1888856ec709"
      },
      "outputs": [],
      "source": [
        "dataset['train'][0], dataset['train'][1], dataset['train'][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "293acab4-d55c-464e-ac88-9c0fa9ecf431",
      "metadata": {
        "id": "293acab4-d55c-464e-ac88-9c0fa9ecf431"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    temp = tokenizer(batch['text'],padding=True,truncation=True)\n",
        "    return temp\n",
        "\n",
        "print(tokenize(dataset['train'][:2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb01c03-533e-4276-86ef-b6744203391b",
      "metadata": {
        "id": "1cb01c03-533e-4276-86ef-b6744203391b"
      },
      "outputs": [],
      "source": [
        "encoded_dataset = dataset.map(tokenize,batched=True,batch_size=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5deeb54-4617-44c7-98c7-dea976fbc635",
      "metadata": {
        "id": "f5deeb54-4617-44c7-98c7-dea976fbc635"
      },
      "outputs": [],
      "source": [
        "# label2id, id2label: mapping between labels (0, 1, 2 ...) and label names (love, fear, surprise ...)\n",
        "label2id = {x['label_names']:x['labels'] for x in dataset['train']}\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "\n",
        "label2id, id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d60d7ad-7ae2-4efa-9e2c-bea37d5b15d5",
      "metadata": {
        "id": "4d60d7ad-7ae2-4efa-9e2c-bea37d5b15d5"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel,PeftConfig,get_peft_model,LoraConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cdc9df8-5e66-42ff-a715-abc9d90ce93b",
      "metadata": {
        "id": "5cdc9df8-5e66-42ff-a715-abc9d90ce93b"
      },
      "outputs": [],
      "source": [
        "num_labels = len(label2id)\n",
        "\n",
        "# Where to train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "config = AutoConfig.from_pretrained(model_ckpt,label2id=label2id,id2label=id2label)\n",
        "\n",
        "# to(device) ==> the model will be trained on cuda (GPU)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,config=config).to(device)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    # Rank: original matrix decomposed into 32 rows and columns\n",
        "    r=32,\n",
        "    # lora_alpha: scaling factor\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules = [\"query\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model,peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1ab4713-91db-4437-b643-6900e48a08d0",
      "metadata": {
        "id": "a1ab4713-91db-4437-b643-6900e48a08d0"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "training_dir = \"bert_base_train_dir\"\n",
        "\n",
        "training_args = TrainingArguments(output_dir=training_dir,\n",
        "                                  overwrite_output_dir = True,\n",
        "                                  num_train_epochs = 5,\n",
        "                                  learning_rate = 2e-5,\n",
        "                                  per_device_train_batch_size = batch_size,\n",
        "                                  per_device_eval_batch_size = batch_size,\n",
        "                                  weight_decay = 0.01,\n",
        "                                  evaluation_strategy = 'epoch',\n",
        "                                  disable_tqdm = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7658d0de-e5ed-48a4-b026-980a7e3b54cc",
      "metadata": {
        "id": "7658d0de-e5ed-48a4-b026-980a7e3b54cc"
      },
      "outputs": [],
      "source": [
        "# Build compute metrics function\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels,preds)\n",
        "\n",
        "    return {\"accuracy\":acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b4ba28-5d97-42df-bbaa-647c502fb684",
      "metadata": {
        "id": "a1b4ba28-5d97-42df-bbaa-647c502fb684"
      },
      "outputs": [],
      "source": [
        "# Use data_collector to convert the samples to PyTorch tensors and concatenate them with the correct amount of padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                  args=training_args,\n",
        "                  compute_metrics=compute_metrics,\n",
        "                  train_dataset=encoded_dataset['train'],\n",
        "                  eval_dataset=encoded_dataset['validation'],\n",
        "                  tokenizer=tokenizer,\n",
        "                  data_collator=data_collator)\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a903d7-ccc0-4434-9922-79d51dbbc608",
      "metadata": {
        "id": "49a903d7-ccc0-4434-9922-79d51dbbc608"
      },
      "outputs": [],
      "source": [
        "preds_output = trainer.predict(encoded_dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f7837b-aa8d-4892-a72c-614e8d3a7a3e",
      "metadata": {
        "id": "e7f7837b-aa8d-4892-a72c-614e8d3a7a3e"
      },
      "outputs": [],
      "source": [
        "preds_output.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167722be-53e0-4f23-b72c-a996d101cab7",
      "metadata": {
        "id": "167722be-53e0-4f23-b72c-a996d101cab7"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(preds_output.predictions,axis=1)\n",
        "\n",
        "y_true = encoded_dataset['test'][:]['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66633044-a31e-4356-9e31-d21b85b4299c",
      "metadata": {
        "id": "66633044-a31e-4356-9e31-d21b85b4299c"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true,y_pred,target_names=list(label2id)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cabae877-50a4-4891-98f4-4294244988bc",
      "metadata": {
        "id": "cabae877-50a4-4891-98f4-4294244988bc"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true,y_pred)\n",
        "\n",
        "ax = sns.heatmap(cm,annot=True)\n",
        "ax.xaxis.set_ticklabels(['fake news','real news'])\n",
        "ax.yaxis.set_ticklabels(['fake news','real news'])\n",
        "ax.set_title('Confusion Matrix - HF Transformers',fontsize=18)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_news():\n",
        "    user_input = input(\"\\nEnter a news headline to check if it's FAKE or REAL: \")\n",
        "    cleaned_input = clean_text(user_input)\n",
        "    prediction = model_NB.predict([cleaned_input])[0]\n",
        "    label = \"REAL\" if prediction == 1 else \"FAKE\"\n",
        "    print(f\"\\nðŸ”Ž **Prediction:** This news is **{label}**!\")\n",
        "\n",
        "# Run the prediction function\n",
        "predict_news()"
      ],
      "metadata": {
        "id": "k7Dg6HAUQcqh"
      },
      "id": "k7Dg6HAUQcqh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V_VvSPYBjhyf"
      },
      "id": "V_VvSPYBjhyf"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nLfdSl78QpQn"
      },
      "id": "nLfdSl78QpQn",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}